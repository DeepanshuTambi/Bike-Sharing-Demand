{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fancyimpute import KNN   \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sn\n",
    "from random import randrange, uniform\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pylab\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from random import *    \n",
    "from numpy import *\n",
    "from scipy.stats import chisquare\n",
    "from sklearn.linear_model import LinearRegression\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Data\n",
    "original_data = pd.read_csv('Data_Project_1.csv', encoding = ' iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = original_data.reset_index(drop=True)\n",
    "original_data.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data=original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=working_data.drop(['dteday','instant'],axis=1)\n",
    "working_data=working_data.drop(['dteday'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat=df.copy()\n",
    "df_num=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting variables to categoric and numeric\n",
    "num_names=['temp','atemp','hum','windspeed','casual','registered','cnt']\n",
    "cat_names=['yr','season','mnth','holiday','weekday','workingday','weathersit']\n",
    "\n",
    "df_num_sliced = df_num.loc[:,num_names]\n",
    "df_cat_sliced = df_num.loc[:,cat_names]\n",
    "\n",
    "for var in cat_names:\n",
    "    df_cat[var] = df_cat[var].astype(\"category\")\n",
    "    df_cat_sliced[var] = df_cat_sliced[var].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing value analysis\n",
    "missing_val = pd.DataFrame(df.isnull().sum())\n",
    "missing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the Raw Data\n",
    "fig,(ax1,ax2,ax3,ax4, ax5, ax6)= plt.subplots(nrows=6)\n",
    "fig.set_size_inches(13,25)\n",
    "cnt_by_mnth = pd.DataFrame(df_cat.groupby(\"mnth\")[\"cnt\"].mean()).reset_index()\n",
    "sn.barplot(data=cnt_by_mnth,x=\"mnth\",y=\"cnt\",ax=ax1)\n",
    "\n",
    "cnt_by_season = pd.DataFrame(df_cat.groupby([\"season\"])[\"cnt\"].mean()).reset_index()\n",
    "sn.barplot(data=cnt_by_season,x=\"season\",y=\"cnt\",ax=ax2)\n",
    "\n",
    "cnt_by_weekday = pd.DataFrame(df_cat.groupby(\"weekday\")[\"cnt\"].mean()).reset_index()\n",
    "sn.barplot(data=cnt_by_weekday,x=\"weekday\",y=\"cnt\",ax=ax3)\n",
    "\n",
    "cnt_by_weathersit = pd.DataFrame(df_cat.groupby(\"weathersit\")[\"cnt\"].mean()).reset_index()\n",
    "sn.barplot(data=cnt_by_weathersit,x=\"weathersit\",y=\"cnt\",ax=ax4)\n",
    "\n",
    "cnt_by_yr = pd.DataFrame(df_cat.groupby(\"yr\")[\"cnt\"].mean()).reset_index()\n",
    "sn.barplot(data=cnt_by_yr,x=\"yr\",y=\"cnt\",ax=ax5)\n",
    "\n",
    "transformed = pd.melt(df_cat[[\"yr\",\"casual\",\"registered\"]], id_vars=['yr'], value_vars=['casual', 'registered'])\n",
    "cnt_by_user= pd.DataFrame(transformed.groupby([\"yr\",\"variable\"],sort=True)[\"value\"].mean()).reset_index()\n",
    "sn.pointplot(x=cnt_by_user[\"yr\"], y=cnt_by_user[\"value\"],hue=cnt_by_user[\"variable\"],hue_order=[\"casual\",\"registered\"], data=transformed, join=True,ax=ax6)\n",
    "ax6.set(xlabel='Year', ylabel='Users Count',label='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier analysis\n",
    "#visualizing outliers (boxplot)\n",
    "fig, axes = plt.subplots(nrows=3,ncols=2)\n",
    "fig.set_size_inches(12,10)\n",
    "sn.boxplot(y='hum', x='mnth',data=df_cat, width=.5,palette=\"colorblind\",ax=axes[0][0])\n",
    "sn.boxplot(y='windspeed', x='mnth',data=df_cat, width=0.5,palette=\"colorblind\",ax=axes[0][1])\n",
    "sn.boxplot(y='atemp', x='mnth',data=df_cat, width=0.5,palette=\"colorblind\",ax=axes[1][0])\n",
    "sn.boxplot(y='cnt', x='mnth',data=df_cat, width=0.5,palette=\"colorblind\",ax=axes[1][1])\n",
    "sn.boxplot(y='casual', x='mnth',data=df_cat, width=0.5,palette=\"colorblind\",ax=axes[2][0])\n",
    "sn.boxplot(y='registered', x='mnth',data=df_cat, width=0.5,palette=\"colorblind\",ax=axes[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing distribution \n",
    "ax8 = original_data.plot(kind='scatter',x='instant', y='registered')\n",
    "ax9 = original_data.plot(kind='scatter',x='instant', y='casual')\n",
    "ax10 = original_data.plot(kind='scatter',x='instant', y='cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms before removing outliers\n",
    "a4_dims = (14, 14)\n",
    "fig, axes = plt.subplots(nrows=3,ncols=2,figsize=a4_dims)\n",
    "sn.set(color_codes=True)\n",
    "sn.set(style=\"white\", palette=\"muted\")\n",
    "sn.distplot(df_num['hum'], ax=axes[0][0])\n",
    "sn.distplot(df_num['atemp'], ax=axes[0][1])\n",
    "sn.distplot(df_num['registered'], ax=axes[1][0])\n",
    "sn.distplot(df_num['windspeed'], ax=axes[1][1])\n",
    "sn.distplot(df_num['cnt'], ax=axes[2][0])\n",
    "sn.distplot(df_num['casual'], ax=axes[2][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting outlier and replacing it with NA\n",
    "\n",
    "q75_w, q25_w = np.percentile(df['windspeed'], [75 ,25])\n",
    "\n",
    "\n",
    "iqr_w = q75_w - q25_w\n",
    "\n",
    "\n",
    "minimum_w = q25_w - (iqr_w*1.5)\n",
    "maximum_w = q75_w + (iqr_w*1.5)\n",
    "\n",
    "q75_h, q25_h = np.percentile(df['hum'], [75 ,25])\n",
    "\n",
    "\n",
    "iqr_h = q75_h - q25_h\n",
    "\n",
    "\n",
    "minimum_h = q25_h - (iqr_h*1.5)\n",
    "maximum_h = q75_h + (iqr_h*1.5)\n",
    "\n",
    "q75_cnt, q25_cnt = np.percentile(df['casual'], [75 ,25])\n",
    "\n",
    "\n",
    "iqr_cnt = q75_cnt - q25_cnt\n",
    "\n",
    "\n",
    "minimum_cnt = q25_cnt - (iqr_cnt*1.5)\n",
    "maximum_cnt = q75_cnt + (iqr_cnt*1.5)\n",
    "\n",
    "q75_atemp, q25_atemp = np.percentile(df_num['atemp'], [75 ,25])\n",
    "\n",
    "\n",
    "iqr_atemp = q75_atemp - q25_atemp\n",
    "\n",
    "\n",
    "minimum_atemp = q25_atemp - (iqr_atemp*1.5)\n",
    "maximum_atemp = q75_atemp + (iqr_atemp*1.5)\n",
    "\n",
    "\n",
    "df_cat.loc[df_cat['hum'] < minimum_h, 'hum'] = np.nan\n",
    "df_cat.loc[df_cat['hum'] > maximum_h, 'hum'] = np.nan\n",
    "df_num.loc[df_num['hum'] < minimum_h, 'hum'] = np.nan\n",
    "df_num.loc[df_num['hum'] > maximum_h, 'hum'] = np.nan\n",
    "df_cat.loc[df_cat['windspeed'] < minimum_w, 'windspeed'] = np.nan\n",
    "df_cat.loc[df_cat['windspeed'] > maximum_w, 'windspeed'] = np.nan\n",
    "df_num.loc[df_num['windspeed'] < minimum_w, 'windspeed'] = np.nan\n",
    "df_num.loc[df_num['windspeed'] > maximum_w, 'windspeed'] = np.nan\n",
    "df_cat.loc[df_cat['casual'] < minimum_cnt, 'casual'] = np.nan\n",
    "df_cat.loc[df_cat['casual'] > maximum_cnt, 'casual'] = np.nan\n",
    "df_num.loc[df_num['casual'] < minimum_cnt, 'casual'] = np.nan\n",
    "df_num.loc[df_num['casual'] > maximum_cnt, 'casual'] = np.nan\n",
    "df_cat.loc[df_cat['atemp'] < minimum_atemp, 'atemp'] = np.nan\n",
    "df_cat.loc[df_cat['atemp'] > maximum_atemp, 'atemp'] = np.nan\n",
    "df_num.loc[df_num['atemp'] < minimum_atemp, 'atemp'] = np.nan\n",
    "df_num.loc[df_num['atemp'] > maximum_atemp, 'atemp'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Calculating missing value after outlier analysis\n",
    "missing_val = pd.DataFrame(df_cat.isnull().sum())\n",
    "missing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat=df_cat.dropna()\n",
    "df_num=df_num.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing before and after outlier analysis scatter plot\n",
    "df_cat_sc= df_cat\n",
    "df_cat_sc['instant']= original_data['instant']\n",
    "ax9_new = df_cat_sc.plot(kind='scatter',x='instant', y='casual', color=\"red\")\n",
    "ax10_new = df_cat_sc.plot(kind='scatter',x='instant', y='cnt')\n",
    "ax9 = original_data.plot(kind='scatter',x='instant', y='casual',color=\"red\")\n",
    "ax10 = original_data.plot(kind='scatter',x='instant', y='cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat=df_cat.drop(['instant'],1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the data that has been removed by deleting outliers of variable casual\n",
    "df.loc[df['casual'] > 2266]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of outliers had only cost major drop to the variable 'weekday'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seeing the effect on weekday before and after outlier play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.groupby([\"yr\",\"weekday\"])[\"cnt\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"yr\",\"weekday\"])[\"cnt\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cat['weekday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms after removing outliers\n",
    "a4_dims = (14, 14)\n",
    "fig, axes = plt.subplots(nrows=3,ncols=2,figsize=a4_dims)\n",
    "sn.set(color_codes=True)\n",
    "sn.set(style=\"white\", palette=\"muted\")\n",
    "sn.distplot(df_num['hum'], ax=axes[0][0])\n",
    "sn.distplot(df_num['atemp'], ax=axes[0][1])\n",
    "sn.distplot(df_num['registered'], ax=axes[1][0])\n",
    "sn.distplot(df_num['windspeed'], ax=axes[1][1])\n",
    "sn.distplot(df_num['cnt'], ax=axes[2][0])\n",
    "sn.distplot(df_num['casual'], ax=axes[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.groupby([\"season\"])[\"mnth\"].unique().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier analysis\n",
    "#visualizing outliers through boxplot after droppping\n",
    "fig, axes = plt.subplots(nrows=2,ncols=2)\n",
    "fig.set_size_inches(12, 10)\n",
    "sn.boxplot(y='hum', x='mnth',data=df_cat, width=0.5,palette=\"colorblind\",ax=axes[0][0])\n",
    "sn.boxplot(y='windspeed', x='mnth',data=df_cat, width=0.5,palette=\"colorblind\",ax=axes[0][1])\n",
    "sn.boxplot(y='atemp', x='mnth',data=df_cat, width=0.5,palette=\"colorblind\",ax=axes[1][0])\n",
    "sn.boxplot(y='casual', x='mnth',data=df_cat, width=0.5,palette=\"colorblind\",ax=axes[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_sliced = df_num.iloc[:,7:15]\n",
    "df_cat_sliced = df_cat.iloc[:,0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_sliced.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for var in cat_names:\n",
    " #   df_cat[var] = df_cat[var].astype(\"category\")\n",
    "  #  df_cat_sliced[var] = df_cat_sliced[var].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_num = df_num.reset_index(drop=True)\n",
    "df_num.index += 1\n",
    "df_cat = df_cat.reset_index(drop=True)\n",
    "df_cat.index += 1\n",
    "df_num_sliced = df_num_sliced.reset_index(drop=True)\n",
    "df_num_sliced.index += 1\n",
    "df_cat_sliced = df_cat_sliced.reset_index(drop=True)\n",
    "df_cat_sliced.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the target continious variable to categorical variable by binning for various tests\n",
    "bins = [0,1000, 2000,3000, 4000,5000, 6000,7000, 8000,9000]\n",
    "labels = [1,2,3,4,5,6,7,8,9]\n",
    "df_cat['cnt_binned'] = pd.cut(df_num['cnt'], bins=bins, labels=labels)\n",
    "df_num['cnt_binned'] = pd.cut(df_num['cnt'], bins=bins, labels=labels)\n",
    "df_cat_sliced['cnt_binned'] = pd.cut(df_num['cnt'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection using Correlation, ANOVA, and Chi-Square test\n",
    "\n",
    "## 1) Correlation\n",
    "\n",
    "correlations = df_num_sliced.corr()\n",
    "correlations\n",
    "correlations.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) Chi-square test of each variable with other (When dependent variable has been converted to categorical through binning)\n",
    "\n",
    "\n",
    "factors_paired_bin = [(i,j) for i in df_cat_sliced.columns.values for j in df_cat_sliced.columns.values] \n",
    "\n",
    "chi2_bin, p_values_bin =[], []\n",
    "\n",
    "for f in factors_paired_bin:\n",
    "    if f[0] != f[1]:\n",
    "             chitest_bin = chi2_contingency(pd.crosstab(df_cat_sliced[f[0]], df_cat_sliced[f[1]]))   \n",
    "             chi2_bin.append(chitest_bin[0])\n",
    "             p_values_bin.append(chitest_bin[1])\n",
    "            \n",
    "    else:      \n",
    "             chi2_bin.append(0)\n",
    "             p_values_bin.append(0)\n",
    "\n",
    "chi2_bin = np.array(chi2_bin).reshape((8,8))\n",
    "chi2_df_bin = pd.DataFrame(chi2_bin, index=df_cat_sliced.columns.values, columns=df_cat_sliced.columns.values)\n",
    "p_values_bin = np.array(p_values_bin).reshape((8,8)) # shaping it as a matrix\n",
    "p_values_bin = pd.DataFrame(p_values_bin, index=df_cat_sliced.columns.values, columns=df_cat_sliced.columns.values)\n",
    "p_values_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOVA Analysis\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "cw_lm=ols('cnt ~ C(yr)+C(holiday)+C(workingday)+ C(mnth)+C(weekday)+ C(weathersit)+C(season)', data=df_cat).fit() \n",
    "print(sm.stats.anova_lm(cw_lm, typ=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the amount of data for category holiday\n",
    "df_cat['holiday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the importance of variable holiday\n",
    "df_cat.groupby([\"yr\",\"holiday\"])[\"cnt\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of data through bar graphs after removing outliers\n",
    "fig,(ax1,ax2,ax3,ax4, ax5, ax6)= plt.subplots(nrows=6)\n",
    "fig.set_size_inches(11,20)\n",
    "cnt_by_mnth = pd.DataFrame(df_cat.groupby(\"mnth\")[\"cnt\"].mean()).reset_index()\n",
    "sn.barplot(data=cnt_by_mnth,x=\"mnth\",y=\"cnt\",ax=ax1)\n",
    "\n",
    "cnt_by_season = pd.DataFrame(df_cat.groupby(\"season\")[\"cnt\"].mean()).reset_index()\n",
    "sn.barplot(data=cnt_by_season,x=\"season\",y=\"cnt\",ax=ax2)\n",
    "\n",
    "cnt_by_weekday = pd.DataFrame(df_cat.groupby(\"weekday\")[\"cnt\"].mean()).reset_index()\n",
    "sn.barplot(data=cnt_by_weekday,x=\"weekday\",y=\"cnt\",ax=ax3)\n",
    "\n",
    "cnt_by_weathersit = pd.DataFrame(df_cat.groupby(\"weathersit\")[\"cnt\"].mean()).reset_index()\n",
    "sn.barplot(data=cnt_by_weathersit,x=\"weathersit\",y=\"cnt\",ax=ax4)\n",
    "\n",
    "cnt_by_yr = pd.DataFrame(df_cat.groupby(\"yr\")[\"cnt\"].mean()).reset_index()\n",
    "sn.barplot(data=cnt_by_yr,x=\"yr\",y=\"cnt\",ax=ax5)\n",
    "\n",
    "transformed = pd.melt(df_cat[[\"yr\",\"casual\",\"registered\"]], id_vars=['yr'], value_vars=['casual', 'registered'])\n",
    "cnt_by_user= pd.DataFrame(transformed.groupby([\"yr\",\"variable\"],sort=True)[\"value\"].mean()).reset_index()\n",
    "sn.pointplot(x=cnt_by_user[\"yr\"], y=cnt_by_user[\"value\"],hue=cnt_by_user[\"variable\"],hue_order=[\"casual\",\"registered\"], data=transformed, join=True,ax=ax6)\n",
    "ax6.set(xlabel='Year', ylabel='Users Count',label='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnth_names = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "#weekday_names = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "#season_names = [\"Spring\",\"Summer\",\"fall\", \"winter\"]\n",
    "#weather_types = [\"lev_a\",\"lev_b\",\"lev_c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary variables according to the model\n",
    "\n",
    "drop_features_for_lm=[\"casual\",\"registered\",\"temp\"]\n",
    "drop_features_for_dt_classification_after_chi=[\"casual\",\"registered\",\"temp\"]\n",
    "drop_features_for_rf_classification_after_chi=[\"casual\",\"registered\",\"temp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Splitting data into train and test\n",
    "random.seed(20)\n",
    "train_num, test_num = train_test_split(df_cat, test_size=0.2)\n",
    "train_num.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df5tr['yr'] = df5tr['yr'].astype('category')\n",
    "#df5tr['holiday'] = df5tr['holiday'].astype('category')\n",
    "#df5tr['workingday'] = df5tr['workingday'].astype('category')\n",
    "#df5te['yr'] = df5te['yr'].astype('category')\n",
    "#df5te['holiday'] = df5te['holiday'].astype('category')\n",
    "#df5te['workingday'] = df5te['workingday'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train_num.drop(['casual','registered','temp'], axis=1)\n",
    "test_num = test_num.drop(['casual','registered','temp'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Decision tree regressor\n",
    "                 \n",
    "                 #a)Creating no bins for  categorical variables, and keeping numerical and other variables as it is\n",
    "        \n",
    "fit_dt = DecisionTreeRegressor(max_depth=10).fit(train_num.iloc[:,0:10], train_num.iloc[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt = fit_dt.predict(test_num.iloc[:,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred): \n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true))*100\n",
    "    \n",
    "    return mape\n",
    "\n",
    "MAPE(test_num.iloc[:,10], predictions_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the reason for high MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({'pred':predictions_dt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_1 = dataset\n",
    "col_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_2=test_num.iloc[:,10].to_frame()\n",
    "col2=col_2.reset_index().drop('index',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_2=col_2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_2=col_2.drop('index',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col= pd.DataFrame(columns=['subt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col['subt']= (col_2['cnt']-col_1['pred'])/(col_2['cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Values of absolute percentage error\n",
    "new_col = new_col.sort_values('subt', ascending = True)\n",
    "new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_num.loc[df_num['casual'] > 2200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num= df_num.sort_values('cnt', ascending = True)\n",
    "df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking for the unnatural data \n",
    "df_cat.loc[df_cat['casual'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the row from dataset\n",
    "df_num=df_num[df_num.casual != 2]\n",
    "df_cat=df_cat[df_cat.casual != 2]\n",
    "\n",
    "\n",
    "train_num=train_num[train_num.cnt!=22]\n",
    "\n",
    "test_num=test_num[test_num.cnt!=22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remodelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Decision tree regressor\n",
    "                 \n",
    "                 #a)Creating no bins for  categorical variables, and keeping numerical and other variables as it is\n",
    "        \n",
    "fit_dt = DecisionTreeRegressor(max_depth=10).fit(train_num.iloc[:,0:10], train_num.iloc[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt = fit_dt.predict(test_num.iloc[:,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred): \n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true))*100\n",
    "    \n",
    "    return mape\n",
    "\n",
    "MAPE(test_num.iloc[:,10], predictions_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dot file to visualise tree  #http://webgraphviz.com/\n",
    "\n",
    "dotfile = open(\"pro.dot\", 'w')\n",
    "dt_export = tree.export_graphviz(fit_dt, out_file=dotfile, feature_names = train_num_vis.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGRESSION MODELS\n",
    "\n",
    "# 2) Linear regression: \n",
    "\n",
    "    #a)Creating dummies for each categorical variable, and keeping numerical variable as it is\n",
    "    \n",
    "weathersit_dummy = pd.get_dummies(df_num.weathersit) \n",
    "season_dummy = pd.get_dummies(df_num.season) \n",
    "mnth_dummy = pd.get_dummies(df_num.mnth) \n",
    "weekday_dummy = pd.get_dummies(df_num.weekday)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnth_dummy.columns=['mnth_1','mnth_2','mnth_3','mnth_4','mnth_5','mnth_6','mnth_7','mnth_8','mnth_9','mnth_10','mnth_11','mnth_12']\n",
    "weekday_dummy.columns=['weekday_1','weekday_2','weekday_3','weekday_4','weekday_5','weekday_6','weekday_7']\n",
    "season_dummy.columns=['season_1','season_2','season_3','season_4']\n",
    "weathersit_dummy.columns=['weathersit_1','weathersit_1','weathersit_3']\n",
    "\n",
    "\n",
    "df2tr = pd.merge(train_num, season_dummy ,left_index=True, right_index=True)\n",
    "df3tr = pd.merge(df2tr, weathersit_dummy ,left_index=True, right_index=True)\n",
    "df4tr = pd.merge(df3tr, weekday_dummy ,left_index=True, right_index=True)\n",
    "df5tr = pd.merge(df4tr, mnth_dummy ,left_index=True, right_index=True)\n",
    "\n",
    "df2te = pd.merge(test_num, season_dummy ,left_index=True, right_index=True)\n",
    "df3te = pd.merge(df2te, weathersit_dummy ,left_index=True, right_index=True)\n",
    "df4te = pd.merge(df3te, weekday_dummy ,left_index=True, right_index=True)\n",
    "df5te = pd.merge(df4te, mnth_dummy ,left_index=True, right_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5tr = df5tr.drop(['season','mnth','weekday','weathersit','casual','registered','temp'], axis=1)\n",
    "df5te = df5te.drop(['season','mnth','weekday','weathersit','casual','registered','temp'], axis=1)\n",
    "df5tr=df5tr[df5tr.cnt != 22]\n",
    "df5te=df5te[df5te.cnt != 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5tr['count']=df5tr['cnt']\n",
    "df5tr['cnt_bin_temp']=df5tr['cnt_binned']\n",
    "df5tr=df5tr.drop(['cnt','cnt_binned'],1)\n",
    "df5te['count']=df5te['cnt']\n",
    "df5te['cnt_bin_temp']=df5te['cnt_binned']\n",
    "df5te=df5te.drop(['cnt','cnt_binned'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5tr = df5tr.rename(columns={'count': 'cnt', 'cnt_bin_temp': 'cnt_binned'})\n",
    "df5te = df5te.rename(columns={'count': 'cnt', 'cnt_bin_temp': 'cnt_binned'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5tr=df5tr.astype('float')\n",
    "df5te=df5te.astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_dummies = sm.OLS(df5tr.iloc[:,32], df5tr.iloc[:,0:32].astype('float')).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_dummies.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr_dummies = model_lr_dummies.predict(df5te.iloc[:,0:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred): \n",
    "    mape_lr_dummies = np.mean(np.abs((y_true - y_pred) / y_true))*100\n",
    "    return mape_lr_dummies\n",
    "#Calculate MAPE\n",
    "MAPE(df5te.iloc[:,32], predictions_lr_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Linear regression: \n",
    "\n",
    "    #b)Creating dummies for only ordinal variable, and keeping numerical and other variables as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weathersit_dummy = pd.get_dummies(df_num.weathersit)\n",
    "weathersit_dummy.columns=['weathersit_1','weathersit_2','weathersit_3']\n",
    "df5tror = pd.merge(train_num, weathersit_dummy ,left_index=True, right_index=True)\n",
    "df5tror=df5tror.drop(['cnt','cnt_binned','weathersit'], 1)\n",
    "df5tror['cnt']=df5tr['cnt']\n",
    "df5tror['cnt_binned']=df5tr['cnt_binned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5teor = pd.merge(test_num, weathersit_dummy ,left_index=True, right_index=True)\n",
    "df5teor=df5teor.drop(['cnt','cnt_binned','weathersit'], 1)\n",
    "df5teor['cnt']=df5te['cnt']\n",
    "df5teor['cnt_binned']=df5te['cnt_binned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df5tr['yr'] = df5tr['yr'].astype('category')\n",
    "#df5tr['holiday'] = df5tr['holiday'].astype('category')\n",
    "#df5tr['workingday'] = df5tr['workingday'].astype('category')\n",
    "#df5te['yr'] = df5te['yr'].astype('category')\n",
    "#df5te['holiday'] = df5te['holiday'].astype('category')\n",
    "#df5te['workingday'] = df5te['workingday'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_or_dummies = sm.OLS(df5tror.iloc[:,12], df5tror.iloc[:,0:12].astype(float)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_or_dummies.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr_or_dummies = model_lr_or_dummies.predict(df5teor.iloc[:,0:12])\n",
    "#df_num.sort_values('cnt',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE(df5teor.iloc[:,12], predictions_lr_or_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Linear regression: \n",
    "\n",
    "    #c)Creating no bins for  categorical variables, and keeping numerical and other variables as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(train_num.iloc[:,10], train_num.iloc[:,0:10].astype(float)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_num.iloc[:,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred): \n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true))*100\n",
    "    return mape\n",
    "#Calculate MAPE\n",
    "MAPE(test_num.iloc[:,10], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 3) Random forest regressor\n",
    "                 \n",
    "                 #a)Creating bins for only ordinal variable, and keeping numerical and other variables as it is\n",
    "rf_model_or = RandomForestRegressor(n_estimators = 20).fit(df5tror.iloc[:,0:12],df5tror.iloc[:,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions_or = rf_model_or.predict(df5teor.iloc[:,0:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE(df5teor.iloc[:,12] , rf_predictions_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Random forest regressor\n",
    "                 \n",
    "                 #b)Creating no bins for  categorical variables, and keeping numerical and other variables as it is\n",
    "rf_model = RandomForestRegressor(n_estimators = 80).fit(train_num.iloc[:,0:10], train_num.iloc[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = rf_model.predict(test_num.iloc[:,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE(test_num.iloc[:,10],rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_vis=train_num.drop(['cnt','cnt_binned'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot of the threshold trees required for random forest\n",
    "estimators = np.arange(10, 400, 10)\n",
    "scores = []\n",
    "for n in estimators:\n",
    "    rf_model.set_params(n_estimators=n)\n",
    "    rf_model.fit(train_num.iloc[:,0:10],train_num.iloc[:,10])\n",
    "    scores.append(rf_model.score(test_num.iloc[:,0:10],test_num.iloc[:,10]))\n",
    "plt.title(\"Effect of n_estimators\")\n",
    "plt.xlabel(\"n_estimator\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(estimators, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) KNN \n",
    "\n",
    "KNN_model = KNeighborsRegressor(n_neighbors = 5).fit(train_num.iloc[:,0:10], train_num.iloc[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_Predictions = KNN_model.predict(test_num.iloc[:,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE(test_num.iloc[:,10], KNN_Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Modelling by imputing outliers through random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Imputation of outliers by categories' means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting outliers and replacing with NAs\n",
    "\n",
    "q75_w, q25_w = np.percentile(df['windspeed'], [75 ,25])\n",
    "\n",
    "\n",
    "iqr_w = q75_w - q25_w\n",
    "\n",
    "\n",
    "minimum_w = q25_w - (iqr_w*1.5)\n",
    "maximum_w = q75_w + (iqr_w*1.5)\n",
    "\n",
    "q75_h, q25_h = np.percentile(df['hum'], [75 ,25])\n",
    "\n",
    "\n",
    "iqr_h = q75_h - q25_h\n",
    "\n",
    "\n",
    "minimum_h = q25_h - (iqr_h*1.5)\n",
    "maximum_h = q75_h + (iqr_h*1.5)\n",
    "\n",
    "q75_cnt, q25_cnt = np.percentile(df['casual'], [75 ,25])\n",
    "\n",
    "\n",
    "iqr_cnt = q75_cnt - q25_cnt\n",
    "\n",
    "\n",
    "minimum_cnt = q25_cnt - (iqr_cnt*1.5)\n",
    "maximum_cnt = q75_cnt + (iqr_cnt*1.5)\n",
    "\n",
    "\n",
    "\n",
    "df.loc[df['hum'] < minimum_h, 'hum'] = np.nan\n",
    "df.loc[df['hum'] > maximum_h, 'hum'] = np.nan\n",
    "df.loc[df['windspeed'] < minimum_w, 'windspeed'] = np.nan\n",
    "df.loc[df['windspeed'] > maximum_w, 'windspeed'] = np.nan\n",
    "df.loc[df['casual'] < minimum_cnt, 'casual'] = np.nan\n",
    "df.loc[df['casual'] > maximum_cnt, 'casual'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing means of weekday category to cnt\n",
    "df.loc[(df['weekday'] == 6) & pd.isnull(df['casual']) , 'cnt'] = 5732 \n",
    "df.loc[(df['weekday'] == 0) & pd.isnull(df['casual']) , 'cnt'] = 5036  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('casual', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rest of the NAs from hum and windspeed as they are randomly distributed\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['atemp','registered'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20)\n",
    "\n",
    "train_num_imp, test_num_imp = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_imp = RandomForestRegressor(n_estimators = 51).fit(train_num_imp.iloc[:,0:10], train_num_imp.iloc[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions_imp = rf_model_imp.predict(test_num_imp.iloc[:,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE(test_num_imp.iloc[:,10],rf_predictions_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot of the threshold trees required for re-modelled random forest\n",
    "estimators = np.arange(10, 400, 10)\n",
    "scores = []\n",
    "for n in estimators:\n",
    "    rf_model.set_params(n_estimators=n)\n",
    "    rf_model.fit(train_num_imp.iloc[:,0:10],train_num_imp.iloc[:,10])\n",
    "    scores.append(rf_model.score(test_num_imp.iloc[:,0:10],test_num_imp.iloc[:,10]))\n",
    "plt.title(\"Effect of n_estimators\")\n",
    "plt.xlabel(\"n_estimator\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(estimators, scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
